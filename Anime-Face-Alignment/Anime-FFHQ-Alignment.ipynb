{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Anime : FFHQ-Alignment\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- [`Happy-jihye`](https://github.com/happy-jihye)\n",
    "- [`happy-jihye/GAN`](https://github.com/happy-jihye/GAN)"
   ],
   "metadata": {
    "id": "qJGZfGf3NN-u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reference\n",
    "\n",
    "- **landmark detector** : \n",
    "  1. [`1adrianb/face-alignment`](https://github.com/1adrianb/face-alignment) : `pip install face-alignment`\n",
    "  2. [`nagadomi/lbpcascade_animeface`](https://github.com/nagadomi/lbpcascade_animeface)\n",
    "  3. [`kanosawa/anime_face_landmark_detection`](https://github.com/kanosawa/anime_face_landmark_detection)\n",
    "  \n",
    "  \n",
    "- **FFHQ alignment** : [NVlabs/ffhq-dataset](https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py)\n",
    "  "
   ],
   "metadata": {
    "id": "qJGZfGf3NN-u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import PIL.Image\n",
    "import face_alignment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import imageio\n",
    "import natsort"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Using `face-alignment`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "image_path = 'raw_image'\n",
    "image_path_list = [os.path.join(image_path, im) for im in os.listdir(image_path) if im.endswith('.jpg')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 FFHQ-Alignment & Save Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from ffhq_align import image_align_68\n",
    "\n",
    "landmark_path = f'align_image'\n",
    "os.makedirs(landmark_path, exist_ok=True)\n",
    "\n",
    "landmarks_detector = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False)\n",
    "print(f'total : {len(image_path_list)}')\n",
    "\n",
    "error = []\n",
    "i = 0\n",
    "\n",
    "for image_path in image_path_list:\n",
    "    face_landmarks = landmarks_detector.get_landmarks(image_path)\n",
    "    \n",
    "    if face_landmarks is None:\n",
    "        error.append(image_path)\n",
    "        continue        \n",
    "\n",
    "    aligned_face_path = os.path.join(landmark_path, f'align-{str(i).zfill(4)}.png')\n",
    "\n",
    "    image_align_68(image_path, aligned_face_path, face_landmarks[0])\n",
    "    i+=1\n",
    "\n",
    "print(f'success : {i}')\n",
    "print(f'fail : {len(image_path_list) - i}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total : 7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/opt/conda/lib/python3.8/site-packages/face_alignment/api.py:132: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success : 4\n",
      "fail : 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Write Errors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "txt = open(f'error.txt', 'w')\n",
    "for er in error:\n",
    "    txt.write(er + '\\n')\n",
    "txt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 (Option) Save images with the landmark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "landmark_path = f'landmark'\n",
    "\n",
    "os.makedirs(landmark_path, exist_ok=True)\n",
    "\n",
    "print(f'total : {len(image_path_list)}')\n",
    "i = 0\n",
    "\n",
    "for image_path in image_path_list:\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n",
    "\n",
    "    input = io.imread(image_path)\n",
    "    preds = fa.get_landmarks(input)\n",
    "\n",
    "    det = fa.get_landmarks_from_image(image_path)\n",
    "    plt.imshow(input) \n",
    "    \n",
    "    if det == None:\n",
    "        plt.close()\n",
    "        continue\n",
    "    \n",
    "    for detection in det:\n",
    "        scat = plt.scatter(detection[:,0], detection[:,1], 2)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(landmark_path + f'/landmark-{str(i).zfill(4)}.png', bbox_inches='tight', pad_inches=0)\n",
    "    scat.remove()\n",
    "    plt.close()\n",
    "    i += 1\n",
    "    \n",
    "print(f'success : {i}')\n",
    "print(f'fail : {len(image_path_list) - i}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total : 7\n",
      "success : 4\n",
      "fail : 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Using `face-alignment`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from ffhq_align import CFA, image_align_24"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Download the cascade file and checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "!wget https://raw.githubusercontent.com/nagadomi/lbpcascade_animeface/master/lbpcascade_animeface.xml"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-08-04 16:01:32--  https://raw.githubusercontent.com/nagadomi/lbpcascade_animeface/master/lbpcascade_animeface.xml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 246945 (241K) [text/plain]\n",
      "Saving to: ‘lbpcascade_animeface.xml’\n",
      "\n",
      "lbpcascade_animefac 100%[===================>] 241.16K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-08-04 16:01:32 (2.28 MB/s) - ‘lbpcascade_animeface.xml’ saved [246945/246945]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "drive_id = '1NckKw7elDjQTllRxttO87WY7cnQwdMqz'\n",
    "\n",
    "url = f'https://drive.google.com/file/d/{drive_id}/view' \n",
    "id= url.split('/')[-2]\n",
    "!gdown --id $id "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NckKw7elDjQTllRxttO87WY7cnQwdMqz\n",
      "To: /workspace/notebooks/FFHQ-Alignmnet/checkpoint_landmark_191116.pth.tar\n",
      "16.7MB [00:00, 37.8MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Read Errors of 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "f = open(f'error.txt', 'r')\n",
    "\n",
    "error = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: \n",
    "        break   \n",
    "    error.append(line.strip('\\n'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 FFHQ-Alignment & Save Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# path\n",
    "landmark_path = f'align_iamge'\n",
    "os.makedirs(landmark_path, exist_ok=True)\n",
    "\n",
    "# param\n",
    "num_landmark = 24\n",
    "img_width = 256\n",
    "checkpoint_name = 'checkpoint_landmark_191116.pth.tar'\n",
    "\n",
    "# detector\n",
    "face_detector = cv2.CascadeClassifier('lbpcascade_animeface.xml')\n",
    "landmark_detector = CFA(output_channel_num=num_landmark + 1, checkpoint_name=checkpoint_name).cuda()\n",
    "\n",
    "# transform\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                   std=[0.5, 0.5, 0.5])\n",
    "train_transform = [transforms.ToTensor(), normalize]\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "\n",
    "error2 = []\n",
    "k = 0\n",
    "for error_image in error:\n",
    "\n",
    "    # input image & detect face\n",
    "    input_img_name = error_image\n",
    "    img = cv2.imread(input_img_name)\n",
    "    faces = face_detector.detectMultiScale(img)\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if faces == ():\n",
    "        error2.append(error_image)\n",
    "        continue\n",
    "\n",
    "    for x_, y_, w_, h_ in faces:\n",
    "\n",
    "        # adjust face size\n",
    "        x = max(x_ - w_ / 8, 0)\n",
    "        rx = min(x_ + w_ * 9 / 8, img.width)\n",
    "        y = max(y_ - h_ / 4, 0)\n",
    "        by = y_ + h_\n",
    "        w = rx - x\n",
    "        h = by - y\n",
    "\n",
    "\n",
    "        # transform image\n",
    "        img_tmp = img.crop((x, y, x+w, y+h))\n",
    "        img_tmp = img_tmp.resize((img_width, img_width), Image.BICUBIC)\n",
    "\n",
    "        img_tmp_tf = train_transform(img_tmp)\n",
    "        img_tmp_tf = img_tmp_tf.unsqueeze(0).cuda()\n",
    "\n",
    "        # estimate heatmap\n",
    "        heatmaps = landmark_detector(img_tmp_tf)\n",
    "        heatmaps = heatmaps[-1].cpu().detach().numpy()[0]\n",
    "\n",
    "        # landmark array\n",
    "        landmark_array = []\n",
    "\n",
    "        # calculate landmark position\n",
    "        for i in range(num_landmark):\n",
    "            heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n",
    "            landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n",
    "            landmark_y = landmark[0] \n",
    "            landmark_x = landmark[1] \n",
    "\n",
    "            # draw landmarks\n",
    "            #draw.ellipse((landmark_x - 2, landmark_y - 2, landmark_x + 2, landmark_y + 2), fill=(255, 0, 0))\n",
    "            landmark_array.append([landmark_x - 2, landmark_y - 2, landmark_x + 2, landmark_y + 2])\n",
    "\n",
    "    # output image\n",
    "    #img_tmp.save(f'./LineWebtoonCharacterDataset/MyDeepestSecret/Emma/align2-{str(k).zfill(4)}.png')\n",
    "\n",
    "    image_align_24(img_tmp, f'{landmark_path}/align2-{str(k).zfill(4)}.png',landmark_array , transform_size=256 )\n",
    "    k+=1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-10-49b5d9d6da77>:31: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Crop Images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "image_path = 'raw_image'\n",
    "image_path_list = [os.path.join(image_path, im) for im in os.listdir(image_path) if im.endswith('.jpg')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# path\n",
    "crop_path = f'crop'\n",
    "os.makedirs(crop_path, exist_ok=True)\n",
    "\n",
    "# param\n",
    "img_width = 256\n",
    "\n",
    "# detector\n",
    "face_detector = cv2.CascadeClassifier('lbpcascade_animeface.xml')\n",
    "\n",
    "k = 0\n",
    "for image_path in image_path_list:\n",
    "\n",
    "    # input image & detect face\n",
    "    img = cv2.imread(image_path)\n",
    "    faces = face_detector.detectMultiScale(img)\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if faces == ():\n",
    "        continue\n",
    "\n",
    "    for x_, y_, w_, h_ in faces:\n",
    "\n",
    "        # adjust face size\n",
    "        x = max(x_ - w_ / 8, 0)\n",
    "        rx = min(x_ + w_ * 9 / 8, img.width)\n",
    "        y = max(y_ - h_ / 4, 0)\n",
    "        by = y_ + h_\n",
    "        w = rx - x\n",
    "        h = by - y\n",
    "\n",
    "        # crop image\n",
    "        img = img.crop((x, y, x+w, y+h))\n",
    "        img = img.resize((img_width, img_width), Image.BICUBIC)\n",
    "\n",
    "    # output image\n",
    "    img.save(f'{crop_path}/crop-{str(k).zfill(4)}.png')\n",
    "\n",
    "    k+=1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-13-6f40fbf7c57d>:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 (Option) Save images with the landmark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from ffhq_align import CFA, image_align_24\n",
    "\n",
    "# path\n",
    "landmark_path = f'./LineWebtoonCharacterDataset/{webtoon}/{name}/landmark'\n",
    "os.makedirs(landmark_path, exist_ok=True)\n",
    "\n",
    "# param\n",
    "num_landmark = 24\n",
    "img_width = 256\n",
    "checkpoint_name = 'checkpoint_landmark_191116.pth.tar'\n",
    "\n",
    "# detector\n",
    "face_detector = cv2.CascadeClassifier('lbpcascade_animeface.xml')\n",
    "landmark_detector = CFA(output_channel_num=num_landmark + 1, checkpoint_name=checkpoint_name).cuda()\n",
    "\n",
    "# transform\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                   std=[0.5, 0.5, 0.5])\n",
    "train_transform = [transforms.ToTensor(), normalize]\n",
    "train_transform = transforms.Compose(train_transform)\n",
    "\n",
    "error2 = []\n",
    "print(f'total : {len(error)}')\n",
    "k = 0\n",
    "for error_image in error:\n",
    "\n",
    "    # input image & detect face\n",
    "    input_img_name = error_image\n",
    "    img = cv2.imread(input_img_name)\n",
    "    faces = face_detector.detectMultiScale(img)\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    #draw = ImageDraw.Draw(img)\n",
    "\n",
    "    if faces == ():\n",
    "        error2.append(error_image)\n",
    "        continue\n",
    "\n",
    "    for x_, y_, w_, h_ in faces:\n",
    "\n",
    "        # adjust face size\n",
    "        x = max(x_ - w_ / 8, 0)\n",
    "        rx = min(x_ + w_ * 9 / 8, img.width)\n",
    "        y = max(y_ - h_ / 4, 0)\n",
    "        by = y_ + h_\n",
    "        w = rx - x\n",
    "        h = by - y\n",
    "        \n",
    "        # transform image\n",
    "        img_tmp = img.crop((x, y, x+w, y+h))\n",
    "        img_tmp = img_tmp.resize((img_width, img_width), Image.BICUBIC)\n",
    "        \n",
    "        draw = ImageDraw.Draw(img_tmp)\n",
    "\n",
    "        img_tmp_tf = train_transform(img_tmp)\n",
    "        img_tmp_tf = img_tmp_tf.unsqueeze(0).cuda()\n",
    "\n",
    "        # estimate heatmap\n",
    "        heatmaps = landmark_detector(img_tmp_tf)\n",
    "        heatmaps = heatmaps[-1].cpu().detach().numpy()[0]\n",
    "\n",
    "        # calculate landmark position\n",
    "        for i in range(num_landmark):\n",
    "            heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n",
    "            landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n",
    "            landmark_y = landmark[0] \n",
    "            landmark_x = landmark[1] \n",
    "\n",
    "            # draw landmarks\n",
    "            draw.ellipse((landmark_x - 2, landmark_y - 2, landmark_x + 2, landmark_y + 2), fill=(255, 0, 0))\n",
    "\n",
    "    # output image\n",
    "    img_tmp.save(f'{landmark_path}/landmark2-{str(k).zfill(4)}.png')\n",
    "\n",
    "    k+=1\n",
    "\n",
    "print(f'success : {k}')\n",
    "print(f'fail : {len(error) - k}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total : 114\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-160-65edddf11f14>:40: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success : 25\n",
      "fail : 89\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "txt = open(f'./LineWebtoonCharacterDataset/{webtoon}/{name}/error2.txt', 'w')\n",
    "for er in error2:\n",
    "    txt.write(er + '\\n')\n",
    "txt.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPWWhR+nzwazNEMYQ629vkX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Face Image Alignment",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}